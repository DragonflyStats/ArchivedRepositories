__Natural Language Processing__

***Laura McCarthy***

<img src="https://github.com/ULStats/MA4128Assessment-2018/blob/master/NLP/Natural-Language-Processing-Definition-1.jpg">


===============================
## Introduction

Natural Language Processing (NLP) is an area of artificial intelligence, concerned with the interactions between computers and human language. It is the application of computational techniques to the analysis and synthesis of natural language and speech.  In layman’s terms Natural Language Processing is the ability of a computer program to understand human language, ether spoken or written. 

## History

Traditionally humans “spoke” to computers through a computer programming language such as Java or C++, NLP looks to change that. There are many difficulties with NLP the biggest being that slang and social context change with language almost every day.  
NLP first started to be developed in the 1950s. Alan Turing developed the Turing Test which was a Georgetown experiment that attempted to translate 60 Russian sentences to English in 1954. As this was at the height of the Cold War, software like this would have been a huge advantage to whoever produced it first. It was thought that progress would be quick with this kind of software but it was not and the first successful NLP systems were not perfected until the 1970s. These included SHRDLU and ELIZA.

Until the 1980s most Natural Language Processing was based on hand written code, but the introduction of machine based learning revolutionised the algorithms for language processing. Both an increase in computational power and a decrease in the domination of Chomskyan theories of Linguistics also aided the increase in success of NLP. 

## How it works

From that time up until now most NLP relies in machine learning using Stastical Inference. Previously direct hand coding was used which was cumbersome and slow. The learning procedures used during machine learning automatically focus on the most common cases, where with handwritten code these cases would not always be as obvious. The automatic learning processes can use statistical inference to produce models that deal with unfamiliar input (new words and sentence structures) as well as spelling and grammatical errors. 

<img src="https://github.com/ULStats/MA4128Assessment-2018/blob/master/NLP/Semantics%20vs%20syntax.jpg">

NLP learns with two main areas, much like a child learning to read. These are Syntax and Semantics. Syntax includes Morphological Segmentation, which is separating the words into individual morphemes. For example the morpheme “ed” at the end of a word would imply a past tense verb, wanted, liked, cited. English Language has quite a simple morphology but languages like Turkish have a very different one.  Other speed-bumps in the Syntax category include “Part-of-Speech tagging” which focuses on the meaning of the word in speech, e.g. book could be a noun or a verb and Word Segmentation which focuses on separating chunks of text into separate words. Again this is quite simple with English Language but far more difficult with languages such as Chinese.

Semantics focuses on the actual meaning of words as opposed to the grammatical structures of them. Again in a language like English it is quite easy to distinguish proper nouns (places, names etc.) as they all begin with a capital letter. However non-western languages are very different and so the code must account for that. 

<img src="https://github.com/ULStats/MA4128Assessment-2018/blob/master/NLP/Chinese%20NLP.gif">
