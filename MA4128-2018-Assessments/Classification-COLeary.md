This essay will focus on forms of data analytics, specifically classification methods. It will give particular focus to classification trees with also an analysis of clustering analysis and Neighbours-based classification. This essay will give more of a focus on how they were developed and their uses more so than the mathematics they are based on. Any reference to statistical software will be towards SPSS, however there will be no SPSS output in this essay.

There are many growing tree algorithms for decision trees. The first regression tree algorithm was published in 1963 in the journal article “Problems in the analysis of survey data and a proposal” in the Journal of the American Statistical Association. The algorithm was called Automatic Interaction Detection (AID) (Morgan & Sonquist, 1963). The next advance in the field came from another journal article in Journal of the American Statistical Association published in 1972. The algorithm developed was called THeta Automatic Interaction Detection (THAID) and it extended the idea to classification (Messenger & Mandell, 1972). However, neither garnered much interest with in the statistical community. There are four decision tree growing algorithms available in SPSS C&RT, CHAID, Exhaustive CHAID and QUEST. The publication of “Classification and Regression Trees” in 1984 by Breidman et al was of paramount importance in regenerating interest in the subject, especially its development of the algorithm C&RT (Breidman, Friedman, Olshen, & Stone, 1984). It follows the same search approach as AID and THAID but adds several improvements. The innovative pruning procedure based on the idea of weakest link cutting solved the over fitting issues of AID and THAID. Although, at the expense of computational cost (Loh, Fifthy Years of Classification and Regression Trees, 2014).CHi-squared Automatic Interaction Detector (CHAID) – an offshoot of AID had some important modifications including built in significance testing (using the most significant predictor rather than most explanatory), multi way splits and a new type of predictor for handling missing information (Kass, 1980). EXHAUSTIVE CHAID is a modification of CHAID, it explores all possible splits for each predictor. (IBM Corporation, 2012)  Quick, Unbiased and Efficient Statistical Tree (QUEST) was developed in 1997 in the journal article “SPLIT SELECTION METHODS FOR CLASSIFICATION TREES” (Loh & Shih, Split Selection Methods For Classification Trees, 1997). QUEST share many similarities with the algorithm FACT. However, it removes bias that can occur for categorical data in FACT. Furthermore, it displays substantial computational advantage over C&RT when there are categorical variables with many values (Loh, Fifthy Years of Classification and Regression Trees, 2014).

A fast, statistical, multi-way tree algorithm that explores data quickly and efficiently and builds segments and profiles with respect to desired outcome. CHAID partition the data into mutually exclusive subsets constructed using a small group of predictors. As previously mentioned, it has in built significant testing and uses the most significant predictor rather than the most explanatory (Kass, 1980). The significance level can be altered but the default value is 0.05. For splitting nodes, the value must be between 0 and 1. For merging categories the value must be greater than 0 and less than or equal to 1. The chi squared test statistic is used to determine node splitting for ordinal dependent variables. For category merging, the likelihood ratio method is used. While, for nominal dependent either the Pearson or likelihood method can be used. Exhaustive CHAID explore all possible splits for each predictor, it is a modification of CHAID (IBM Corporation, 2012). It should be notes that both CHAID and Exhaustive CHAID accept only nominal or ordinal categorical predictors. Their algorithm consists of three steps: merging, splitting and stopping. However, if a predictor is continuous it can be transformed into ordinal predictors before using the algorithm (Baizyldayeva, Uskenbayeva, & Amanzholova, 2013).

A complete binary tree algorithm, which partitions data and produces accurate homogeneous subsets. The algorithm sets out to maximise within node homogeneity. The extent of which a node does not represent a homogenous subset of cases is an indication of impurity. For scale dependent variables, least-squared deviation (LSD) is used as a measure of impurity. For categorical data, either GINI, Twoing or ordered twoing (only ordinal variables) measures of impurity can be used. The minimum change in improvement is equivalent to the minimum decrease in impurity, the default values is 0.0001 (IBM Corporation, 2012). Another characteristic of the growing method is “surrogate” splits. These are splits made on alternate variables to substitute for the preferred split when the preferred split is inapplicable due to missing values (Loh, Fifthy Years of Classification and Regression Trees, 2014). 

A statistical algorithm that selects variables without bias and builds accurate binary trees quickly and efficiently (IBM Corporation, 2012).  The split strategy for QUEST is similar to FACT. QUEST is shown to be faster than exhaustive search algorithms (which also tend to be biased towards selecting variables that afford more splits) and classification accuracy of the trees are typically comparable to those of an exhaustive search (Loh & Shih, Split Selection Methods For Classification Trees, 1997). 

Cluster analysis is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value

The three forms of cluster analysis being examined in this essay is hierarchical, k-means and two step clustering. The basic process of hierarchical clustering was defined by S.C. Johnson in 1967 in his article Hierarchical Clustering Schemes in Psychometrika (Johnson, 1967). The roots of K-means clustering goes back as far as 1957 with developments from Hugo Steinhaus and Stuart Lloyd. However, the term “k-means” only came into existence in 1967, when it was used by James McQueen. (MacQueen, 1967) In 1993 Banfield and Raftery introduced a model-based distance measure for data with continuous attributes in their journal article Model-based Gaussian and non-Gaussian clustering (Banfield & Raftery, 1993). Meila and Heckerman applied this probabilistic concept and derived another distance measure for data with categorical attributes only. (Meila & Heckerman, 1998) The SPSS TwoStep Cluster Component extends this model-based distance measure to situations that include both continuous and categorical variables. (SPSS, 2001)

The basic steps in cluster analysis is as follows:

Step One:
Similarities are formed between all pairs of the objects based on a given attribute.

Step Two:
Groups are constructed so that within-group similarities are larger than the between-group similarities.

Let d(i,k) be a proximity between the ith and kth pattern, with the following satisfied:

	For a dissimilarity d(i,i) = 0 for all i.
	For a similarity, d(i,i) = max d(i,k), for all i,k
	d(i,k) = d(k,i) for all i,k
	d(i,k) ≥ 0, for all i,k 
(King, 2015)

This is just a basic algorithm for clustering. Each clustering algorithm differs. For example Hierarchical clustering can either be Agglomerative Hierarchical clustering or divisive hierarchical clustering. For Agglomerative Hierarchical clustering, you begin with N clusters and continue clustering until you have only 1 cluster. For divisive clustering you begin with 1 cluster and continue clustering until you have N clusters. N being the number of observations in the sample. For k-means clustering it can be split into two phases; The initialization phase and the iteration phase. In the first phase cases are randomly assigned to k cluster. In the second phase the distance between each case and each cluster is computed and cases are assigned to the nearest cluster. Two step then is a mixture of them both.

Another form of classification methods is Neighbours-based classification. It is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbours of the point

In conclusion, all forms of classification have their own merits. Decision trees are used when the outcome is binary. Classification trees are best used when there are a vide range of outcomes for example grouping customers who are most likely to select a number of different insurance policies. Neighbours-based classifications it simpler than the other two. Data analyis, specifically for this essay classification methods, are of paramount importance. Applying any of these models to the data that is created daily can produce insights into the world around. As the world continues to grow, the importance of the classification methods will continue to grow.

